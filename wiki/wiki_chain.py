import os
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from vectordb.chroma_store import ChromaDBManager
from wiki.wiki_fetcher import WikiFetcher
import torch 
import logging
import time
import numpy as np

logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(__name__)

class WikiSummarizer:
    def __init__(
        self,
        model_name: str = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B",
        embed_func=ChromaDBManager()
    ):
        logger.info("Initializing WikiSummarizer...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.token = os.getenv("HUGGINGFACE_API_KEY")  # 토큰 직접 불러오기
        if not self.token:
          logger.warning("HUGGINGFACE_API_KEY not found in environment variables!")

        logger.info(f"Loading model: {model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            token=self.token  
        ).to(self.device)
        logger.info("Model loaded successfully")
        
        logger.info("Loading tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=self.token
        )
        logger.info("Tokenizer loaded successfully")

        logger.info("Setting up pipeline...")
        self.pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_length=1024,
            # temperature=0.01,
            # do_sample=True,
            device=0 if self.device == "cuda" else -1
        )
        logger.info("Pipeline created")

        self.llm = HuggingFacePipeline(pipeline=self.pipeline)
        self.prompt = PromptTemplate.from_template("요약해줘: {text}")
        self.chain = self.prompt | self.llm
        self.embed_func = embed_func.embed_and_store

        logger.info("WikiSummarizer initialization complete")

    def summarize_wiki(self, state: dict) -> dict:
        logger.info(f"Starting wiki summarization for project_id: {state.get('project_id')}")
        content = state.get("content")
        
        if not content:
            logger.error("'content' key missing in input state")
            raise KeyError("'content' 키가 없습니다.")

        logger.info("Generating summary...")

        # 토큰 길이 검사
        tokens = self.tokenizer.encode(content, add_special_tokens=False)
        max_model_input_tokens = 1024

        # 로깅 변수 초기화
        chunk_strategy = "single_chunk" 
        chunk_overlap = 0 
        avg_chunk_length_actual = len(tokens) 
        total_chunks_in_corpus = 1 
        llm_chunking_time = 0.0
        embedding_generation_time = 0.0

        summary_llm_call_start_time = time.time() 


        if len(tokens) > max_model_input_tokens:
            print(f"Content length {len(tokens)} tokens exceeds max {max_model_input_tokens}. Auto chunking activated.")
            
            chunk_strategy = "tokenizer_based"
            chunk_overlap = 100 

            chunking_start_time = time.time() 
            chunks = [tokens[i:i + max_model_input_tokens - chunk_overlap] for i in range(0, len(tokens), max_model_input_tokens - chunk_overlap)]
            chunk_texts = [self.tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]
            llm_chunking_time = time.time() - chunking_start_time 

            avg_chunk_length_actual = np.mean([len(self.tokenizer.encode(c)) for c in chunk_texts]) if chunk_texts else 0
            total_chunks_in_corpus = len(chunk_texts)
            
            summaries = []
            for i, chunk_text in enumerate(chunk_texts):
                logger.info(f"Summarizing chunk {i + 1}/{total_chunks_in_corpus}...")
                partial_summary = self.chain.invoke({"text": chunk_text})
                summaries.append(partial_summary)

            summary = "\n".join(summaries)
        else:
            summary = self.chain.invoke({"text": content})
            # 청킹이 없는 경우, 위에서 초기화된 avg_chunk_length_actual과 total_chunks_in_corpus 값을 유지

        summary_llm_call_end_time = time.time()
        logger.info(f"Summary generated by LLM in {summary_llm_call_end_time - summary_llm_call_start_time:.2f} seconds")
        logger.info(f"Summary length: {len(summary)} characters")

        metadata = {"project_id": state["project_id"]}
        if state.get("updated_at"):
            metadata["updated_at"] = state["updated_at"]
    
        logger.info(f"Storing summary in vector database with metadata: {metadata}")
        
        # 임베딩 및 DB 저장 시간 측정 
        embedding_start_time = time.time()
        self.embed_func(summary, metadata) 
        embedding_generation_time = time.time() - embedding_start_time
        
        logger.info("Wiki summarization completed successfully")
        return {
            "message": "wiki_saved"
        }


    def summarize_diff_files(self, state: dict) -> dict:
        logger.info(f"Starting wiki summarization for project_id: {state.project_id}")
        fetcher = WikiFetcher(state.project_id, state.url)
        changed_files = fetcher.get_diff_files()
        logger.info(f"Summarizing {len(changed_files)} files")

        all_embedding_log_ids = []

        for path in changed_files:
            with open(path, "r") as f:
                content = f.read()
            
            result = self.summarize_wiki({
                "project_id": state.project_id,
                "content": content,
                "updated_at": state.updated_at,
                "document_path": path,
            })
            if result and result.get("embedding_log_id"):
                all_embedding_log_ids.append(result["embedding_log_id"])

        return {
            "message": f"wiki: {len(changed_files)} files summarized",
            "embedding_log_ids_processed": all_embedding_log_ids # 처리된 ID 목록 반환
        }